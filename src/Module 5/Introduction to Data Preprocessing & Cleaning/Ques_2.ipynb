{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values and their count:\n",
      "Feature1    1\n",
      "Feature2    1\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "Feature1    0\n",
      "Feature2    0\n",
      "Feature3    0\n",
      "Target      0\n",
      "dtype: int64\n",
      "\n",
      "Accuracy without handling missing values: 0.0000\n",
      "\n",
      "Accuracy after handling missing values: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'Feature1': [5, 7, 8, np.nan, 10, 7, 5, 6],\n",
    "    'Feature2': [1, 2, 1, 3, 4, np.nan, 2, 1],\n",
    "    'Feature3': [3, 2, 1, 4, 3, 2, 4, 5],\n",
    "    'Target': [1, 0, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Task 1: Identify columns with missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Columns with missing values and their count:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Task 2: Replace missing values with mean (for numerical columns) or mode (for categorical columns)\n",
    "# Identifying numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Replace missing numerical values with mean\n",
    "df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].mean())\n",
    "\n",
    "# Verify if missing values are handled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Task 3: Compare model performance with and without handling missing values\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model without handling missing values (using the original dataset)\n",
    "df_original = pd.DataFrame(data)  # Load original dataset\n",
    "X_original = df_original.drop('Target', axis=1)\n",
    "y_original = df_original['Target']\n",
    "\n",
    "# Impute missing values in the original dataset before splitting into train/test\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_original_imputed = imputer.fit_transform(X_original)\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_original_imputed, y_original, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model on original dataset (with imputation)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_orig, y_train_orig)\n",
    "y_pred_orig = model.predict(X_test_orig)\n",
    "\n",
    "# Evaluate accuracy on original dataset\n",
    "accuracy_without_handling = accuracy_score(y_test_orig, y_pred_orig)\n",
    "print(f\"\\nAccuracy without handling missing values: {accuracy_without_handling:.4f}\")\n",
    "\n",
    "# Train model after handling missing values (already done in the df)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy after handling missing values\n",
    "accuracy_with_handling = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy after handling missing values: {accuracy_with_handling:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset with duplicates\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5, 1, 2],\n",
    "    'Feature2': [5, 6, 7, 8, 9, 5, 6],\n",
    "    'Feature3': [9, 10, 11, 12, 13, 9, 10],\n",
    "    'Target': [0, 1, 0, 1, 0, 0, 1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Task 1: Identify and remove duplicate entries\n",
    "print(\"Dataset before removing duplicates:\")\n",
    "print(df)\n",
    "\n",
    "# Shape of the dataset before removal\n",
    "print(f\"\\nShape before removing duplicates: {df.shape}\")\n",
    "\n",
    "# Remove duplicate entries\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "\n",
    "# Task 2: Document the shape after removing duplicates\n",
    "print(\"\\nDataset after removing duplicates:\")\n",
    "print(df_no_duplicates)\n",
    "\n",
    "# Shape of the dataset after removal\n",
    "print(f\"\\nShape after removing duplicates: {df_no_duplicates.shape}\")\n",
    "\n",
    "# Task 3: Explain to a classmate how duplicate data can affect prediction accuracy\n",
    "print(\"\\nExplanation: \")\n",
    "print(\"Duplicate data can skew the model training process. It can lead to overfitting by giving certain data points more weight than they deserve. \"\n",
    "      \"This results in a model that performs well on the training data but doesn't generalize well to unseen data, lowering the prediction accuracy. \"\n",
    "      \"Removing duplicates ensures that each data point is counted only once, providing a more accurate and generalizable model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset with incorrect data types\n",
    "data = {\n",
    "    'Feature1': ['1', '2', '3', '4', '5'],  # This column contains numbers as strings\n",
    "    'Feature2': [10.0, 20.0, 30.0, 40.0, 50.0],  # This column is of type float\n",
    "    'Feature3': ['Yes', 'No', 'Yes', 'No', 'Yes'],  # This is categorical data\n",
    "    'Target': ['0', '1', '0', '1', '0']  # Target variable stored as string\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Task 1: Convert 'Feature1' and 'Target' columns from string to integer\n",
    "df['Feature1'] = df['Feature1'].astype(int)  # Convert string numbers to integers\n",
    "df['Target'] = df['Target'].astype(int)  # Convert string target to integers\n",
    "\n",
    "# Task 2: Identify and correct columns with inconsistent data types\n",
    "print(\"Before type correction:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Let's say 'Feature2' should actually be an integer\n",
    "df['Feature2'] = df['Feature2'].astype(int)  # Convert float to integer if needed\n",
    "\n",
    "# Task 2: Check data types after correction\n",
    "print(\"\\nAfter type correction:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Task 3: Explanation\n",
    "print(\"\\nExplanation: \")\n",
    "print(\"Correct data types are crucial for feature engineering because models expect data in a specific format.\")\n",
    "print(\"For example, a model that expects numeric values will fail if provided with strings.\")\n",
    "print(\"Additionally, some machine learning algorithms may require categorical data to be encoded correctly as integers or dummy variables.\")\n",
    "print(\"Correcting data types ensures that operations such as scaling, encoding, and transformations are applied correctly, which is vital for building an accurate model.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example dataset (random data with outliers)\n",
    "np.random.seed(0)\n",
    "data = {\n",
    "    'Feature1': np.random.normal(loc=50, scale=10, size=100).tolist() + [200, 220, 230],  # Adding outliers\n",
    "    'Feature2': np.random.normal(loc=100, scale=20, size=100).tolist() + [500, 550, 600],  # Adding outliers\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Task 1: Visualize the dataset and identify outliers using a boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting boxplots for each feature\n",
    "sns.boxplot(data=df)\n",
    "plt.title('Boxplot to Identify Outliers')\n",
    "plt.show()\n",
    "\n",
    "# Task 2: Remove or adjust outliers using the IQR (Interquartile Range) method\n",
    "Q1 = df.quantile(0.25)\n",
    "Q3 = df.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds for detecting outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter the DataFrame to remove outliers\n",
    "df_cleaned = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]\n",
    "\n",
    "# Plot the cleaned dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df_cleaned)\n",
    "plt.title('Boxplot After Removing Outliers')\n",
    "plt.show()\n",
    "\n",
    "# Task 3: Technique for Handling Outliers (Research and Explanation)\n",
    "print(\"\\nResearch and Report on a Technique for Handling Outliers:\")\n",
    "print(\"One effective method for handling outliers is the Interquartile Range (IQR) method. \"\n",
    "      \"This method involves calculating the first quartile (Q1) and the third quartile (Q3) of the data and using the IQR to define the bounds for detecting outliers. \"\n",
    "      \"Any data points that fall outside these bounds are considered outliers and can be removed or adjusted.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
